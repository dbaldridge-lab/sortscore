"""
Logging module for reproducible analysis.

This module provides RESTful-style logging for sortscore analyses, capturing
complete metadata about analysis requests, execution, and outputs.
"""

import json
import os
import time
import hashlib
import platform
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
from dataclasses import dataclass, field, asdict

import sortscore

logger = logging.getLogger(__name__)


@dataclass
class AnalysisRequest:
    """Captures the analysis request parameters."""
    config_file: str
    cli_args: Dict[str, Any]
    analysis_params: Dict[str, Any]


@dataclass
class ExecutionInfo:
    """Captures execution timing and processing stats."""
    start_time: str
    end_time: Optional[str] = None
    duration_seconds: Optional[float] = None
    variants_processed: Optional[int] = None
    variants_filtered: Optional[int] = None
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)


@dataclass
class OutputFile:
    """Metadata for a single output file."""
    file: str
    path: str
    size_bytes: Optional[int] = None
    checksum: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AnalysisOutputs:
    """All outputs generated by the analysis."""
    scores: Optional[OutputFile] = None
    statistics: Optional[OutputFile] = None
    heatmap: Optional[OutputFile] = None
    heatmap_matrix: Optional[OutputFile] = None
    positional_averages: Optional[OutputFile] = None
    codon_heatmap: Optional[OutputFile] = None
    codon_heatmap_matrix: Optional[OutputFile] = None


@dataclass
class EnvironmentInfo:
    """Environment and system information."""
    sortscore_version: str
    python_version: str
    working_directory: str
    hostname: str
    platform: str


class AnalysisLogger:
    """
    RESTful-style logger for sortscore analyses.
    
    Captures complete metadata about analysis requests, execution, and outputs
    in a structured, machine-readable format suitable for audit trails and
    reproducibility.
    """
    
    def __init__(self, experiment_name: str, suffix: str, output_dir: str):
        """
        Initialize analysis logger.
        
        Parameters
        ----------
        experiment_name : str
            Name of the experiment being analyzed
        suffix : str
            Suffix for output files
        output_dir : str
            Output directory for analysis files
        """
        self.experiment_name = experiment_name
        self.suffix = suffix
        self.output_dir = output_dir
        
        # Generate unique analysis ID
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.analysis_id = f"{experiment_name}_{suffix}_{timestamp}"
        
        # Initialize data structures
        self.request: Optional[AnalysisRequest] = None
        self.execution = ExecutionInfo(
            start_time=datetime.now().isoformat()
        )
        self.outputs = AnalysisOutputs()
        self.environment = self._capture_environment()
        
        # Log file path
        self.log_file = os.path.join(output_dir, f"{experiment_name}_analysis_{suffix}.log.json")
        
        logger.info(f"Analysis logger initialized: {self.analysis_id}")
    
    def set_request(self, config_file: str, cli_args: Dict[str, Any], 
                   analysis_params: Dict[str, Any]) -> None:
        """Set the analysis request information."""
        self.request = AnalysisRequest(
            config_file=config_file,
            cli_args=cli_args,
            analysis_params=analysis_params
        )
    
    def set_processing_stats(self, variants_processed: int, 
                           variants_filtered: int = 0) -> None:
        """Set variant processing statistics."""
        self.execution.variants_processed = variants_processed
        self.execution.variants_filtered = variants_filtered
    
    def add_error(self, error: str) -> None:
        """Add an error message to the log."""
        self.execution.errors.append(error)
        logger.error(f"Analysis error: {error}")
    
    def add_warning(self, warning: str) -> None:
        """Add a warning message to the log."""
        self.execution.warnings.append(warning)
        logger.warning(f"Analysis warning: {warning}")
    
    def log_output_file(self, file_type: str, filename: str, 
                       full_path: str, **metadata) -> None:
        """
        Log an output file with metadata.
        
        Parameters
        ----------
        file_type : str
            Type of file ('scores', 'statistics', 'heatmap', etc.)
        filename : str
            Base filename
        full_path : str
            Full path to the file
        **metadata
            Additional metadata specific to the file type
        """
        # Calculate file size and checksum if file exists
        size_bytes = None
        checksum = None
        if os.path.exists(full_path):
            size_bytes = os.path.getsize(full_path)
            checksum = self._calculate_checksum(full_path)
        
        output_file = OutputFile(
            file=filename,
            path=full_path,
            size_bytes=size_bytes,
            checksum=checksum,
            metadata=metadata
        )
        
        # Set the appropriate output field
        setattr(self.outputs, file_type, output_file)
        logger.info(f"Logged output file: {file_type} -> {filename}")
    
    def finish(self) -> str:
        """
        Finish the analysis and write the complete log.
        
        Returns
        -------
        str
            Path to the generated log file
        """
        # Set end time and calculate duration
        end_time = datetime.now()
        self.execution.end_time = end_time.isoformat()
        
        start_time = datetime.fromisoformat(self.execution.start_time)
        duration = (end_time - start_time).total_seconds()
        self.execution.duration_seconds = round(duration, 2)
        
        # Create complete log structure
        log_data = {
            "analysis_id": self.analysis_id,
            "timestamp": self.execution.end_time,
            "status": "failed" if self.execution.errors else "completed",
            "request": asdict(self.request) if self.request else None,
            "execution": asdict(self.execution),
            "outputs": self._serialize_outputs(),
            "environment": asdict(self.environment)
        }
        
        # Write log file
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
        with open(self.log_file, 'w') as f:
            json.dump(log_data, f, indent=2, default=str)
        
        logger.info(f"Analysis log saved to {self.log_file}")
        return self.log_file
    
    def _capture_environment(self) -> EnvironmentInfo:
        """Capture current environment information."""
        return EnvironmentInfo(
            sortscore_version=getattr(sortscore, '__version__', 'unknown'),
            python_version=platform.python_version(),
            working_directory=os.getcwd(),
            hostname=platform.node(),
            platform=platform.platform()
        )
    
    def _calculate_checksum(self, filepath: str) -> str:
        """Calculate SHA256 checksum of a file."""
        hash_sha256 = hashlib.sha256()
        try:
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()[:16]  # First 16 chars for brevity
        except Exception as e:
            logger.warning(f"Could not calculate checksum for {filepath}: {e}")
            return "unknown"
    
    def _serialize_outputs(self) -> Dict[str, Any]:
        """Serialize outputs, filtering out None values."""
        outputs_dict = asdict(self.outputs)
        return {k: v for k, v in outputs_dict.items() if v is not None}


def generate_date_suffix() -> str:
    """Generate a date-based suffix for output files."""
    return datetime.now().strftime('%Y%m%d')